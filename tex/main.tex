\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{listings}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{Machine Learning}
\author{Sina Aleyaasin}

\begin{document}

\maketitle

\section{Preliminaries}
Computational approaches to $learning$ from data involves some or all of following processes.
\begin{enumerate}
  \item \textbf{Feature engineering} encoding information about a learning problem into machine processable format.
  \item \textbf{Model selection} employing appropriate models to describe data
  \item \textbf{Training} process existing empirical data to make hypotheses that predict future, unknown data points.
\end{enumerate}
Techniques in machine learning coarsely divide into two categories
\begin{itemize}
  \item Supervised learning
  \item Unsupervised learning
\end{itemize}

\subsection{Feature engineering}
Consider the problem of classifying objects from a set of examples $\chi \in X$. 
To represent the problem in the domain of a machine, we must represent the objects using numbers.
\begin{definition}
A \textbf{feature vector} is some vector representation of an object $x$
  \begin{equation}
    x \equiv \phi(\chi) \in \mathbb{R}^{d}
    \label{eqn:featureengineering}
  \end{equation}
\end{definition}
The exercise of selecting $\phi$ is referred to as \textit{feature enginering}.
We may express the set of examples $X$ in this way as a matrix.
\begin{definition}
A \textbf{design matrix} is a matrix of $N$ feature vectors representing $N$ objects 
    \begin{equation}
    X = \begin{bmatrix}
        x^{(1)T} \\
        x^{(2)T} \\
        \vdots \\
        x^{(N)T}
    \end{bmatrix}
    \end{equation}
\end{definition} 


\subsection{Supervised Learning}
Consider the task of modelling the relationship between some dependent variable (\textit{target}) on some explanatory independent variable (\textit{feature}) given a data set.
\begin{definition}
A \textbf{training example} is an ordered \textit{feature-target} pair 
    \begin{equation}
    (x^{(i)}, y^{(i)}) \in \mathbb{X} \times \mathbb{Y}
    \end{equation}
\end{definition}
\begin{definition}
A \textbf{training set} of size $N$ is the set of training examples 
\begin{equation}
    \{(x^{(i)}, y^{(i)})| i = 1, 2, \dots N\}
\end{equation}
\end{definition}
The objective is to learn a \textit{hypothesis}
\begin{equation}
    h : \mathbb{X} \rightarrow \mathbb{Y}
\end{equation}
that models the relationship between the data in the training set and can predict new data points beyond it.
\begin{definition}
$\mathbf{Regression}$ is supervised learning for a continuous, real valued $\mathbb{Y} \equiv \mathbb{R}^{n+1}$.
\end{definition}
\begin{definition}
\textbf{Classification} is supervised learning for a finite, discrete $\mathbb{Y}$. The hypothesis of a classification problem is also known as a \textbf{classifier}.
\end{definition}

\subsection{Model selection}
\todo{$\mathcal{H}$}
\subsection{Training}
\todo{$h\in\mathcal{H}$}

\section{Linear Regression}
Linear regression assumes a linear dependence of the target on the features. 
\begin{equation}
    \label{eqn:linearregression}
    h(x;\theta) \equiv h_{\theta}(x) = \theta^{T}x 
\end{equation}
where $\theta, x \in \mathbb{R}^{n+1}$.
\begin{remark}
Motivated by aesthetics, this notation adheres to the convention that every feature vector $x$ has a constant first element $x_{0}=1$ to account for the intercept term $\theta_{0}$.
\end{remark}
The parameter $\theta$ is determined by minimizing some \textit{loss function} that aims to quantify the ``error'' of the classifier.
We consider here the loss function giving corresponding to the \textbf{ordinary least squares} regression model
\begin{equation}
    \label{eqn:lossfunction}
    J(\theta) = \sum_{i=1}^{N}J^{(i)}(\theta) =  \sum_{i=1}^{N}\frac{1}{2}{\left(y^{(i)} - h_{\theta}(x^{(i)})\right)}^{2}
\end{equation}
\subsection{Minimizing loss by normal equations}
In some instances, as with the least mean squares regression model, the loss function may be minimized analytically to yield a closed formed solution for $\theta$

\todo{theorem and proof of linear algebra + calculate theta}

\subsection{Minimizing loss by gradient descent}
For instances where no closed form solutions exist, one may perform a \textbf{gradient descent} until a desired threshold of convergence is reached.
\begin{equation}
    \theta_{j+1} = \theta_{j} - \alpha \nabla_{\theta}J(\theta)
\end{equation}
The parameter $\alpha$ is known as the \textbf{learning rate}.
This difference equation describes \textbf{batch gradient decent}.
When the training set is large, one common modification can be made
\begin{equation}
    \theta_{j+1} = \theta_{j} - \alpha \nabla_{\theta} J^{(i++)}(\theta_{j})
\end{equation}
where the $++$ operator indicates iteration through the training set with each global iteration. This is referred to as a \textbf{stochastic gradient descent}.

\subsection{Probabilistic interpretation}
In order to appreciate the choice of loss function in the ordinary-least-squares model, consider the ``error'' produced by a linear model in a training example
\begin{align}
    \label{eqn:trainingexampleerror}
    \epsilon^{(i)} &= y^{(i)} - h(x^{i}) \nonumber\\
                &= y^{(i)} - \theta^{T}x^{(i)} \\ \nonumber
\end{align}
Let us assume that the distribution of $\epsilon^{(i)}$ in some training set is independently and identically distributed (IID) according to a Gaussian distribution
\begin{equation}
    p(e^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{e^{(i)2}}{2\sigma^{2}}\right)
\end{equation}
which by (\ref{eqn:trainingexampleerror}) implies
\begin{equation}
    \label{eqn:gaussiandistribution}
    p{(y^{(i)}|x^{(i)}; \theta)} = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{{(y^{(i)} - \theta^{T}x^{(i)})}^{2}}{2\sigma^{2}}\right) \\
\end{equation}
This assumption may alternatively be expressed 
\begin{equation}
    e^{(i)} \sim \mathcal{N}(0, \sigma^{2})
\end{equation}
Equation (\ref{eqn:gaussiandistribution}) states the conditional probability of the random variable $x^{(i)}$ given the random variable $y^{(i)}$, parameterised by $\theta$, takes the form of the given Gaussian distribution.
Minimizing the error amounts to maximizing this probability.
In order to prescribe this probability to a training set, we use matrix notation.
The \textbf{likelihood} function of a training set can then be expressed
\begin{equation}
    L(\theta) = L(\theta; X, \vec{y}) = p (\vec{y}|X;\theta)
\end{equation}
where $\vec{y} = {[y^{(1)}, y^{(2)}, \dots, y^{(N)}]}^{T}$
Using the independence assumption
\begin{align}
    L(\theta) &= \prod_{i=1}^{N}p(y^{(i)}|x^{(i)}; \theta) \nonumber \\
    &= \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{{(y^{(i)} - \theta^{T}x^{(i)})}^{2}}{2\sigma^{2}}\right) 
\end{align}
We node that maximizing the likelihood function is equivalent to maximizing any monotonically increasing function of the likelihood.
We choose the logarithm function\footnote{In computer arithmetic, addition is less expensive that multiplication. 
By using the logarithm function, products become sums and computation efficiency improves.} to yield
\begin{align}
    \ell(\theta) &\equiv \log L(\theta) \nonumber \\
    &= \sum_{i=1}^{N} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{{(y^{(i)} - \theta^{T}x^{(i)})}^{2}}{2\sigma^{2}}\right) \nonumber \\
    &\sim \sum_{i=1}^{N}\frac{1}{2}{\left(y^{(i)} - \theta^{T}x^{(i)}\right)}^{2} \label{eqn:likelihoodfunction}
  \end{align}
Not incidentally, we see from (\ref{eqn:linearregression}) that maximizing the likelihood  (\ref{eqn:likelihoodfunction}) is equivalent to minimizing our loss function defined in (\ref{eqn:lossfunction}).

\subsubsection{Weighted linear regression}
One common modification to the liner regression model is to account for \textit{weights} in the loss function
\begin{equation}
  J(\theta) = \frac{1}{2}\sum_{i=1}^{N}w^{(i)}{\left(y^{(i)} - h_{\theta}(x^{(i)})\right)}^{2}
\end{equation}
A standard choice for $w^{(i)}$ is
\begin{equation}
  w^{(i)} = \exp\left(-\frac{{(x^{(i)} - x)}^{T}(x^{(i)}-x)}{2\tau^{2}}\right)
\end{equation}
where $\tau$ is the \textbf{bandwidth parameter} and \todo{elaborate on $x$} 

\section{Logistic regression}
Logistic regression is an approach to \textit{binary classification} $\mathbb{Y} = \{0,1\}$ using a classifier naturally based on the \textbf{logistic function}.
\begin{equation}
  h{(x; \theta)} = \frac{1}{1+e^{-\theta^{T}x}}
  \label{eqn:logisticclassifier}
\end{equation}
Probabilistically, we interpret $h$ such that 
\begin{equation}
  h_{\theta}(x) = P(y = 1 | x; \theta) 
  \label{eqn:logisticprobabilistic}
\end{equation}
and hence the conditional probability of $y$ given $x$ takes the form
\begin{equation}
  P(y|x; \theta) = h_{\theta}{(x)}^{y} + {\left( 1 - h_{\theta}(x) \right)}^{1-y}
  \label{eqn:logisticlikelihood}
\end{equation}
Extending (\ref{eqn:logisticlikelihood}) for a training set, using the assumption that the target errors are independently and identically distributed 
\begin{align}
  L{(\vec{y})} &= p(X; \theta) \nonumber \\
  &= \prod_{i=1}^{N}{{\left( h_{\theta}(x^{(i)}) \right)}^{y^{(i)}}}{\left( 1 - h_{\theta}(x^{(i)}) \right)}^{(1-y^{(i)})} 
\end{align}
\todo{calculation of log likelihood plus derivative}

\section{Generalised Linear Model}
The models discussed in the previous section may be expressed as subsets of a generalised \textbf{exponential family} of distributions which posit the target error distribution according to an arbitrary statistical model.
\begin{equation}
  p{(y;\eta)} = b{(y)}\exp{(\eta^{T}T(y))-\alpha(\eta)}
  \label{eqn:exponentialfamily}
\end{equation}
\todo{Define terms and express following distributions as GLM }
\subsection{Gaussian distribution}
\subsection{Bernoulli distribution}
\subsection{Multinomial distribution}
\subsection{Poisson distribution}
\subsection{Gamma distribution}
\subsection{Dirichlet distribution}

\section{Constructing a Generalised Linear Model}
\todo{Lay out strategy for designing. Exemplify with OLS, logistic regression, softmax regression.}
\subsection{Ordinary least squares}
\subsection{Logistic regression}
\subsection{Softmax regression}

\section{Generative learning}
A \textit{generative model} aims to model data with a predictive capacity for both the feature and target variables of a data set. This is in contrast to \textit{discriminative} models such as the regression models in the previous section which aim to model the target variable based on input features.
Probabilistically, we may interpret generative learning as determining a probability distribution of $x\in \mathbb{X}$ given $y\in\mathbb{Y}$
\begin{equation}
  p(x|y)
  \label{eqn:generativelearning}
\end{equation}

\subsection{Gaussian discriminative analysis}
\subsection{Naive Bayes}
The Naive Bayes assumption states that the conditional probability of individual elements in a feature set given the target are \textit{independent}.
That is to say, for $x\in\mathbb{R}^\{ n + 1 \}$,
\begin{equation}
  p(x|y) \equiv p(x_{1}, x_{2}, \dots x_{n+1}| y) = \prod_{i = 1}^{n+1}p(x_{i}|y)
  \label{eqn:naivebayesassumption}
\end{equation}
Alternatively, this may be more intuitively expressed 
\begin{equation}
  p(x_{i}|y) = p(x_{i}|y, x_{j}) \qquad \forall i,j \in \{1, 2, \dots, n + 1\}
  \label{eqn:intuitivenaivebayes}
\end{equation}

\subsection{Learning Theory}
\subsubsection{Training error}
The \textit{training error} of a classifier describes its failure ratio in predicting output $y^{(i)}$ in the training set
\begin{equation}
  \xi_{n}(h) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}\left[ h(x^{(i)}) \neq y^{(i)}\right]
\end{equation}
where the operator $\mathbb{I}$ maps a proposition to the binary value corresponding to its truth value.
\[
  \mathbb{I}\left[true\right] = 1 \qquad \mathbb{I}\left[false\right] = 0
\]
A small training error does not necessarily an indication of the quality of a classifier. 
A classifier may have a low training error but may not generalise well to data outside the training set. Such a classifier is the result of \textit{overtraining}.
\subsubsection{Generalization error}
Generalization is the application of a classifier to data beyond the training set. 
The \textit{generalisation error} measures the failure ratio of predicting $y^{(i)}$ outside the training set.
A good quality classifier is one that generalises well.
\begin{equation}
  \xi (h) = \frac{1}{n'} \sum_{i = n}^{n + n'} \left[ h(x^{(i)}) \neq y^{(i)}\right]
\end{equation}

\section{Linear classification}
A linear classifier demarcates two classes within a feature space by means of a hyperplane known as a \textit{decision boundary}.  
Objects are classified according to which side of the hyperplane they lie. 
\begin{definition}
For a feature space $\mathbb{X} \in \mathbb{R}^{d+1}$, a decision boundary is defined by the normal vector $\theta \in \mathbb{R}^{d + 1}$
\begin{equation}
  P_{\theta} = \{ z | z \cdot \theta\ = 0\}  
  \label{eqn:hyperplane}
\end{equation}
\end{definition}

The side to which a point $x \in \mathbb{R}^{d}$ lies may be qualified by 
\begin{equation}
  \text{sign} (x\cdot\theta) \equiv x\cdot \theta / ||x|| ||\theta||
  \label{eqn:sideofplane}
\end{equation}
Thus, we may express an arbitrary binary linear classifier $h : \mathbb{R}^{d+1} \rightarrow \{+1, -1\} $ as
\begin{equation}
  h(x, \theta) = \text{sign}(x\cdot\theta)
  \label{eqn:linearclassifier}
\end{equation}
\subsection{Separability}
A training set $T = \{(x_{i}, y_{i})|i=1,2 \dots n\}$ is said to be \textit{linearly separable} if there exists a decision boundary that correctly classifies every example $(x_{i}, y_{i})$
\begin{equation}
  \exists\theta\forall (y, x)[y(x\cdot \theta) > 0]
  \label{eqn:linearlyseparable}
\end{equation}

\subsection{Mistake-driven algorithms}
Mistake driven algorithms are simple procedures used $learn$ the parameter $\theta$ of a linear model for some training set.

\subsubsection{Perceptron}
The perceptron algorithm iterates through the training set, modifying $\theta$ accordingly each time it miss-classifies a training example.
\begin{lstlisting}[language=Python]
def perceptron(training_set, epochs, init_theta):
    theta = init_theta
    for n in range(epochs):
        for x, y in training_set:
	    if y * theta.dot(x) <= 0:
	    	theta += x
    return theta
\end{lstlisting}
\todo{If training set is linearly separable, perceptron is guaranteed to converge to solution. Prove this!}
\subsection{Loss functions}
In the perceptron algorithm, every misclassification triggers a modification of equal significance to the parameter $\theta$. 
This disregards the distance of the training example from the decision boundary.
A somewhat more sophisticated approach would be to incorporate some \textit{loss function} that accounts for the margin of error in a misclassification an modifies $\theta$ accordingly.

\subsubsection{Hinge Loss}
Hinge loss gives penalizes $\theta$ on each iteration proportionally to the margin of error
\begin{equation}
  \text{Hinge}(x, y; \theta) = \max\{0, 1 - y(x\cdot\theta)\}
  \label{eqn:hingeloss}
\end{equation}
\begin{lstlisting}[language=Python]
def hingeloss(training_set, epochs, init_theta):
    theta = init_theta
    for n in range(epochs):
        for x, y in training_set:
	    theta += max(0, 1 - y * theta.dot(x))
    return theta
\end{lstlisting}
\subsubsection{Passive-aggressive Algorithm}
\todo{Large margin classifiers / Passive aggressive algorithm}
\subsection{Support Vector Machines}

\section{Recommender problems}

\section{Non-linear classification}
One approach to non-linear classification involves performing a linear classification on the set of vectors resulting from some non-linear map of the feature set
\begin{equation}
  \varphi : \mathbb{R}^{d} \rightarrow \mathbb{R}^{k}
  \label{eqn:nonlinearmap}
\end{equation}
where typically $k > d$.
In effect, the training set becomes
\begin{equation}
  T \rightarrow T'_{n} = \{(\varphi(x_{i}), y_{i})|i = 1, 2 \dots n\}
  \label{eqn:modtrainingset}
\end{equation}
The resulting linear decision boundary is mapped into a non-linear boundary after returning to the original coordinates.
\subsection{Kernel methods}
Feature maps in (\ref{eqn:nonlinearmap}) typically increase the dimension of the feature space and thus bear an inflated computational cost.
A method used to address this involves circumventing computations with the high dimensional vectors in favour of \textbf{inner products} between them.  
\begin{equation}
  	\varphi(x)\cdot\varphi(x') = K(x, x')
  \label{eqn:kernelfunction}
\end{equation}
$K$ is known as a \textit{kernel function}. A kernel function is \textit{valid} if there exists some $\varphi$ such that (\ref{eqn:kernelfunction}) holds.

\subsubsection{Kernel perceptron}
The kernel perceptron can be understood by consider $\theta$ expressed in the form 
\begin{equation}
  \theta = \sum_{i=1}^{n}\alpha_{i}y_{i}\varphi(x_{i})
  \label{eqn:kernelperceptron}
\end{equation}
where $\alpha_{i}$ denotes the number of times training example $i$ was missclassified.
Taking the inner product with $\varphi(x_{j})$ on both sides
\begin{equation}
  \theta\cdot\varphi(x_{j}) = \sum_{i=1}^{n} \alpha_{i}K(x_{i}, x_{j})
  \label{eqn:kernelform}
\end{equation}
\todo{kernel perceptron pseudocode}

\subsubsection{Kernel Functions}
\todo{4 theorems for kernel composition}

\section{Neural Networks}
In the context of the methods discussed thus far, learning amounts to obtaining an hypothesis that maps elements in a feature space to predictions in the solution space
\begin{equation}
  h : \mathbb{R}^{d} \rightarrow \mathbb{Y}
  \label{eqn:hypothesis}
\end{equation}
where for regression $\mathbb{Y} = \mathbb{R}$ and for classification $\mathbb{Y}$ is a small and finite.
Neural networks are a class of hypotheses that are the functional composition of multiple \textit{layers}
\begin{equation}
  h = \ell_{1} \circ \ell_{2} \circ \dots \circ \ell_{n}   
  \label{eqn:composition}
\end{equation}
where $n$ is the number of layers.
Each layer is a map 
\begin{equation}
  \ell_{i} : \mathbb{R}^{k_{i - 1}} \rightarrow \mathbb{R} ^ {k_{i}}
  \label{eqn:layers}
\end{equation}
where $k_{i} \in \mathbb{N}$, $k_{0} = d$ and $\mathbb{R}^{k_{n}} \equiv \mathbb{Y}$.
A layer itself is composed of an \textit{activation} together with a transformation function 
\begin{equation}
  \ell_{i} = \alpha_{i} \circ \beta_{i} 
  \label{eqn:layercomposition}
\end{equation}
The transformation function maps the vectors through spaces which, in general, vary in dimension between layers.
\begin{equation}
  \beta_{i} : \mathbb{R}^{k_{i-1}} \rightarrow \mathbb{R}^{k_{i}}
  \label{eqn:transformation}
\end{equation}
Using the notation $\mathbb{M}(m, n)$ to denote an $m \times n$ real matrix, the transformation takes the form
\begin{equation}
  \beta_{i}(x; W_{i}, b_{i}) = W_{i} x + b_{i}
  \label{eqn:transformfunction}
\end{equation}
where the weights  $W_{i}, b_{i}$ are parameters
\begin{equation}
  x \in \mathbb{R}^{k_{i-1}} \quad W_{i} \in \mathbb{M}(k_{i-1}, k_{i}) \quad b_{i}\in \mathbb{R}^{k_{i}} 
  \label{eqn:weights}
\end{equation}
The activation is inspired by and typically mimics the role of \textit{threshold potential} in a biological neuron 
\begin{equation}
  \alpha_{i} : \mathbb{R}^{k_{i}} \rightarrow \mathbb{R}^{k_{i}}
  \label{eqn:activation}
\end{equation}
Examples of commonly used activation functions include the sigmoid function (\ref{eqn:logistic}), ReLU ({\ref{eqn:relu}}), hyperbolic tangent (\ref{eqn:hyperbolictan}).

\begin{equation}
\label{eqn:logistic}
  \alpha(x) &=\frac{1}{1 + \exp(x)}
\end{equation}

\begin{equation}
 \label{eqn:relu} 
  \alpha(x) &= \begin{cases}
    0 \quad \text{for} \quad x \leq 0 \\
    x \quad \text{for} \quad x > 0
  \end{cases}
\end{equation}

\begin{equation}
  \alpha(x) = \text{tanh}(x)
  \label{eqn:hyperbolictan}
\end{equation}
A neural network is parameterized by the set of weights 
\begin{equation}
  \Theta = \{W_{i}, b_{i}| i = 1, 2, \dots n\}
  \label{eqn:weights}
\end{equation}
The set of hypotheses for a particular neural network can may compactly be expressed
\begin{equation}
  \mathcal{H} = h(x;\Theta) 
  \label{eqn:neuralnetwork}
\end{equation}
Finding a $h \in \mathcal{H}$ that generalises well from a given training set amounts learning the parameter $\Theta$.
This is the objective of \textit{training}.
\subsection{Training neural networks}
\end{document}

